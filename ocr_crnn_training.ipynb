{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SEALiu/ocr/blob/main/ocr_crnn_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FIHDP6Ba05T"
   },
   "source": [
    "### CRNN Model with CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: tensorflow: command not found\n"
     ]
    }
   ],
   "source": [
    "!tensorflow --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.crnn import CRNN\n",
    "from models.loss import CTCLoss\n",
    "from models.accuracy import WordACC\n",
    "\n",
    "from config import CRNNConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CRNNConfig()\n",
    "\n",
    "table = tf.lookup.StaticHashTable(tf.lookup.TextFileInitializer(\n",
    "    cfg.TABLE_PATH,\n",
    "    tf.string,\n",
    "    tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "    tf.int64,\n",
    "    tf.lookup.TextFileIndex.LINE_NUMBER), cfg.NUM_CLASSES - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img, mode='training'):\n",
    "    assert mode in ['training', 'validation', 'inference'], 'error'\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    if mode == 'train':\n",
    "        img_shape = (32, 320, 3)\n",
    "        # 饱和度\n",
    "        img = tf.image.random_saturation(img, lower=0, upper=3)\n",
    "        # 色调 randomly picked in the interval [-max_delta, max_delta)\n",
    "        img = tf.image.random_hue(img, max_delta=0.3)\n",
    "        # 对比度\n",
    "        img = tf.image.random_contrast(img, lower=0.5, upper=5)\n",
    "        # 亮度\n",
    "        img = tf.image.random_brightness(img, max_delta=0.05)\n",
    "\n",
    "    elif mode == 'validation':\n",
    "        img_shape = (32, 320, 3)\n",
    "    else:\n",
    "        img_shape = (32, 720, 3)\n",
    "\n",
    "    img = img / 255\n",
    "    img -= 0.5\n",
    "    img /= 0.5\n",
    "\n",
    "    h, w, c = img_shape\n",
    "    resized_img = tf.image.resize(img, (h, w), preserve_aspect_ratio=True)\n",
    "    return tf.image.pad_to_bounding_box(resized_img, 0, 0, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    return tf.image.decode_jpeg(img, channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_label(img, label):\n",
    "    chars = tf.strings.unicode_split(label, \"UTF-8\")\n",
    "    tokens = tf.ragged.map_flat_values(table.lookup, chars)\n",
    "    tokens = tokens.to_sparse()\n",
    "    return img, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_img(path, label, mode='training'):\n",
    "    img = tf.io.read_file(path)\n",
    "    return process_img(img, mode), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    获取图片的路径、标签的路径\n",
    "    \"\"\"\n",
    "    dir_path = cfg.TRAIN_DATA_PATH\n",
    "\n",
    "    if os.path.exists(os.path.join(dir_path, 'dataset.data')):\n",
    "        with open(os.path.join(dir_path, 'dataset.data'), 'rb') as ds:\n",
    "            train_all_image_paths, train_all_image_labels, val_all_image_paths, val_all_image_labels = pickle.load(ds)\n",
    "\n",
    "        print('Loaded! Load dataset from dataset.data.')\n",
    "        return train_all_image_paths, train_all_image_labels, val_all_image_paths, val_all_image_labels\n",
    "\n",
    "    img_list = []\n",
    "    train_all_image_paths = []\n",
    "    train_all_image_labels = []\n",
    "    val_all_image_paths = []\n",
    "    val_all_image_labels = []\n",
    "    for root, dirs, files in tqdm(os.walk(dir_path)):\n",
    "        for file in files:\n",
    "            if '.jpg' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                label_path = file_path.replace('.jpg', '.txt')\n",
    "                if Path(file_path.replace('.jpg', '.txt')).exists():\n",
    "                    with open(label_path) as f:\n",
    "                        label = f.read().strip()\n",
    "                    img = cv2.imread(file_path)\n",
    "                    if img.shape[1] / img.shape[0] <= 10 and len(label) > 0:\n",
    "                        img_list.append((file_path, label))\n",
    "\n",
    "    random.shuffle(img_list)\n",
    "    for img, label in img_list:\n",
    "        random_num = random.randint(1, 100)\n",
    "        if random_num == 1:\n",
    "            val_all_image_paths.append(img)\n",
    "            val_all_image_labels.append(label)\n",
    "        else:\n",
    "            train_all_image_paths.append(img)\n",
    "            train_all_image_labels.append(label)\n",
    "\n",
    "    with open(os.path.join(dir_path, 'dataset.data'), 'wb') as ds:\n",
    "        pickle.dump((train_all_image_paths, train_all_image_labels, val_all_image_paths, val_all_image_labels), ds)\n",
    "\n",
    "    print('Loaded.')\n",
    "    return train_all_image_paths, train_all_image_labels, val_all_image_paths, val_all_image_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(cfg.NUM_CLASSES, cfg.INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 320, 64)       1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 160, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 160, 128)      73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 80, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 80, 256)        295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 8, 80, 256)        32        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 80, 256)        590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 8, 82, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 81, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 81, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 81, 512)        16        \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 81, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 4, 83, 512)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 82, 512)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 81, 512)        1049088   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 81, 512)        4         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 5529)        2836377   \n",
      "=================================================================\n",
      "Total params: 11,536,205\n",
      "Trainable params: 11,536,179\n",
      "Non-trainable params: 26\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=cfg.LEARNING_RATE),\n",
    "              loss=CTCLoss(),\n",
    "              metrics=[WordACC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=cfg.LOG_DIR),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(cfg.CHECKPOINT_DIR, 'output/crnn_{epoch}.h5'),\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded! Load dataset from dataset.data.\n",
      "Train Set: 194537\n",
      "Validation Set: 1947\n"
     ]
    }
   ],
   "source": [
    "train_all_image_paths, train_all_image_labels, val_all_image_paths, val_all_image_labels = load_dataset()\n",
    "\n",
    "train_images_num = len(train_all_image_paths)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_all_image_paths, train_all_image_labels))\n",
    "train_ds = train_ds.map(load_and_process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=cfg.BUFFER_SIZE)\n",
    "train_ds = train_ds.repeat()\n",
    "train_ds = train_ds.batch(cfg.BATCH_SIZE)\n",
    "train_ds = train_ds.map(decode_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.apply(tf.data.experimental.ignore_errors())\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_images_num = len(val_all_image_paths)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_all_image_paths, val_all_image_labels))\n",
    "val_ds = val_ds.map(load_and_process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(buffer_size=cfg.BUFFER_SIZE)\n",
    "val_ds = val_ds.repeat()\n",
    "val_ds = val_ds.batch(cfg.BATCH_SIZE)\n",
    "val_ds = val_ds.map(decode_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.apply(tf.data.experimental.ignore_errors())\n",
    "val_ds = val_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print('Train Set:', train_images_num)\n",
    "print('Validation Set:', val_images_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds,\n",
    "          epochs=cfg.EPOCHS,\n",
    "          steps_per_epoch=train_images_num // cfg.BATCH_SIZE,\n",
    "          validation_data=val_ds,\n",
    "          validation_steps=val_images_num // cfg.BATCH_SIZE,\n",
    "          initial_epoch=0,\n",
    "          callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPPwsv2Avfqyo8Z2PQtnP90",
   "include_colab_link": true,
   "name": "ocr_crnn_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
